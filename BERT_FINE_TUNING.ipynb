{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT_BASE_DIR='bert_roman_urdu'\n",
    "\n",
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint $BERT_BASE_DIR/model.ckpt-100000 \\\n",
    "#   --config $BERT_BASE_DIR/config.json \\\n",
    "#   --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained('results_senti_mix/best-checkpoint_f1',num_labels=3)\n",
    "# model = BertForSequenceClassification.from_pretrained('bert_roman_urdu', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert_roman_urdu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "senti_mix_train = pd.read_csv('fine_tuning_data/SentiMix_train_ru.csv')\n",
    "senti_mix_test = pd.read_csv('fine_tuning_data/SentiMix_test_ru.csv')\n",
    "\n",
    "sentiment_categorical = senti_mix_train['sentiment'].astype('category').cat\n",
    "class_names = list(sentiment_categorical.categories)\n",
    "\n",
    "sentences_train = list(senti_mix_train.sentence)\n",
    "sentiment_train = list(sentiment_categorical.codes)\n",
    "\n",
    "X_test = list(senti_mix_test.sentence)\n",
    "y_test = list(senti_mix_test['sentiment'].astype('category').cat.codes)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(sentences_train, sentiment_train, test_size=0.1)\n",
    "\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, add_special_tokens = True, return_attention_mask = True, return_tensors = \"pt\", max_length=128)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, add_special_tokens = True, return_attention_mask = True, return_tensors = \"pt\", max_length=128)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, add_special_tokens = True, return_attention_mask = True, return_tensors = \"pt\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SentiMixDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = SentiMixDataset(train_encodings, y_train)\n",
    "val_dataset = SentiMixDataset(val_encodings, y_val)\n",
    "test_dataset = SentiMixDataset(test_encodings, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/media/usama/48E0E582E0E5769A/Users/AIM-LAB-SERVER/Desktop/results_senti_mix',  # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # total number of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    warmup_steps=60,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir='./logs',  # directory for storing logs\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=EvaluationStrategy.STEPS,\n",
    "    eval_steps = 10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=train_dataset,  # training dataset\n",
    "    eval_dataset=val_dataset,  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 14:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.137700</td>\n",
       "      <td>1.103790</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.318199</td>\n",
       "      <td>0.364762</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>2.493200</td>\n",
       "      <td>681.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.073600</td>\n",
       "      <td>1.070576</td>\n",
       "      <td>0.414706</td>\n",
       "      <td>0.384199</td>\n",
       "      <td>0.424678</td>\n",
       "      <td>0.414706</td>\n",
       "      <td>2.520100</td>\n",
       "      <td>674.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.045300</td>\n",
       "      <td>1.031316</td>\n",
       "      <td>0.474706</td>\n",
       "      <td>0.463536</td>\n",
       "      <td>0.511092</td>\n",
       "      <td>0.474706</td>\n",
       "      <td>2.524600</td>\n",
       "      <td>673.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.996700</td>\n",
       "      <td>0.986562</td>\n",
       "      <td>0.501765</td>\n",
       "      <td>0.502409</td>\n",
       "      <td>0.507116</td>\n",
       "      <td>0.501765</td>\n",
       "      <td>2.515800</td>\n",
       "      <td>675.735000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.937300</td>\n",
       "      <td>0.949216</td>\n",
       "      <td>0.541765</td>\n",
       "      <td>0.540090</td>\n",
       "      <td>0.548061</td>\n",
       "      <td>0.541765</td>\n",
       "      <td>2.509000</td>\n",
       "      <td>677.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.962700</td>\n",
       "      <td>0.909383</td>\n",
       "      <td>0.554118</td>\n",
       "      <td>0.551958</td>\n",
       "      <td>0.553031</td>\n",
       "      <td>0.554118</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>677.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.992800</td>\n",
       "      <td>0.885858</td>\n",
       "      <td>0.562353</td>\n",
       "      <td>0.549311</td>\n",
       "      <td>0.562537</td>\n",
       "      <td>0.562353</td>\n",
       "      <td>2.510200</td>\n",
       "      <td>677.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.860500</td>\n",
       "      <td>0.857858</td>\n",
       "      <td>0.580588</td>\n",
       "      <td>0.577580</td>\n",
       "      <td>0.581693</td>\n",
       "      <td>0.580588</td>\n",
       "      <td>2.529000</td>\n",
       "      <td>672.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.842583</td>\n",
       "      <td>0.598824</td>\n",
       "      <td>0.595259</td>\n",
       "      <td>0.594230</td>\n",
       "      <td>0.598824</td>\n",
       "      <td>2.527900</td>\n",
       "      <td>672.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.945100</td>\n",
       "      <td>0.832873</td>\n",
       "      <td>0.604706</td>\n",
       "      <td>0.601682</td>\n",
       "      <td>0.601114</td>\n",
       "      <td>0.604706</td>\n",
       "      <td>2.530900</td>\n",
       "      <td>671.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.877800</td>\n",
       "      <td>0.845043</td>\n",
       "      <td>0.598235</td>\n",
       "      <td>0.588168</td>\n",
       "      <td>0.600308</td>\n",
       "      <td>0.598235</td>\n",
       "      <td>2.532700</td>\n",
       "      <td>671.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.839700</td>\n",
       "      <td>0.819546</td>\n",
       "      <td>0.613529</td>\n",
       "      <td>0.611139</td>\n",
       "      <td>0.613560</td>\n",
       "      <td>0.613529</td>\n",
       "      <td>2.531900</td>\n",
       "      <td>671.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.781500</td>\n",
       "      <td>0.819191</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>0.611743</td>\n",
       "      <td>0.610910</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>2.535600</td>\n",
       "      <td>670.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.813100</td>\n",
       "      <td>0.820773</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.621026</td>\n",
       "      <td>0.625045</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>2.535900</td>\n",
       "      <td>670.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.786800</td>\n",
       "      <td>0.826658</td>\n",
       "      <td>0.618235</td>\n",
       "      <td>0.616280</td>\n",
       "      <td>0.615959</td>\n",
       "      <td>0.618235</td>\n",
       "      <td>2.534000</td>\n",
       "      <td>670.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.796700</td>\n",
       "      <td>0.824105</td>\n",
       "      <td>0.627647</td>\n",
       "      <td>0.625130</td>\n",
       "      <td>0.626192</td>\n",
       "      <td>0.627647</td>\n",
       "      <td>2.613300</td>\n",
       "      <td>650.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.708100</td>\n",
       "      <td>0.846899</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>0.585669</td>\n",
       "      <td>0.606449</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>2.553000</td>\n",
       "      <td>665.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.902100</td>\n",
       "      <td>0.811676</td>\n",
       "      <td>0.620588</td>\n",
       "      <td>0.623105</td>\n",
       "      <td>0.639104</td>\n",
       "      <td>0.620588</td>\n",
       "      <td>2.662600</td>\n",
       "      <td>638.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.815135</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.632816</td>\n",
       "      <td>0.650698</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>2.800900</td>\n",
       "      <td>606.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.808800</td>\n",
       "      <td>0.809087</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>0.625333</td>\n",
       "      <td>0.626181</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>2.619500</td>\n",
       "      <td>648.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.884400</td>\n",
       "      <td>0.809077</td>\n",
       "      <td>0.633529</td>\n",
       "      <td>0.631522</td>\n",
       "      <td>0.632305</td>\n",
       "      <td>0.633529</td>\n",
       "      <td>2.670000</td>\n",
       "      <td>636.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.827697</td>\n",
       "      <td>0.620588</td>\n",
       "      <td>0.618695</td>\n",
       "      <td>0.622010</td>\n",
       "      <td>0.620588</td>\n",
       "      <td>2.584200</td>\n",
       "      <td>657.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.816800</td>\n",
       "      <td>0.799385</td>\n",
       "      <td>0.642941</td>\n",
       "      <td>0.645052</td>\n",
       "      <td>0.650374</td>\n",
       "      <td>0.642941</td>\n",
       "      <td>2.547900</td>\n",
       "      <td>667.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.797952</td>\n",
       "      <td>0.631765</td>\n",
       "      <td>0.625549</td>\n",
       "      <td>0.626384</td>\n",
       "      <td>0.631765</td>\n",
       "      <td>2.647600</td>\n",
       "      <td>642.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.685300</td>\n",
       "      <td>0.822782</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.619465</td>\n",
       "      <td>0.621253</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>2.541900</td>\n",
       "      <td>668.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.783600</td>\n",
       "      <td>0.851735</td>\n",
       "      <td>0.622353</td>\n",
       "      <td>0.624137</td>\n",
       "      <td>0.636669</td>\n",
       "      <td>0.622353</td>\n",
       "      <td>2.571600</td>\n",
       "      <td>661.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.677500</td>\n",
       "      <td>0.843812</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>0.625868</td>\n",
       "      <td>0.626092</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>2.641000</td>\n",
       "      <td>643.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>0.844206</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.628122</td>\n",
       "      <td>0.631844</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>2.586300</td>\n",
       "      <td>657.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0.833472</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>0.628622</td>\n",
       "      <td>0.632889</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>2.553500</td>\n",
       "      <td>665.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.729600</td>\n",
       "      <td>0.830937</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>0.628780</td>\n",
       "      <td>0.633703</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>2.548500</td>\n",
       "      <td>667.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.838753</td>\n",
       "      <td>0.635882</td>\n",
       "      <td>0.637371</td>\n",
       "      <td>0.643270</td>\n",
       "      <td>0.635882</td>\n",
       "      <td>2.541900</td>\n",
       "      <td>668.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.857858</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>0.607769</td>\n",
       "      <td>0.609485</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>2.617300</td>\n",
       "      <td>649.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.851499</td>\n",
       "      <td>0.612353</td>\n",
       "      <td>0.605327</td>\n",
       "      <td>0.609556</td>\n",
       "      <td>0.612353</td>\n",
       "      <td>2.539700</td>\n",
       "      <td>669.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.643300</td>\n",
       "      <td>0.843805</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.621257</td>\n",
       "      <td>0.620465</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>2.540600</td>\n",
       "      <td>669.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>0.831192</td>\n",
       "      <td>0.624118</td>\n",
       "      <td>0.623818</td>\n",
       "      <td>0.624241</td>\n",
       "      <td>0.624118</td>\n",
       "      <td>2.539600</td>\n",
       "      <td>669.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.806800</td>\n",
       "      <td>0.859696</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>0.609561</td>\n",
       "      <td>0.617097</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>2.542100</td>\n",
       "      <td>668.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.639400</td>\n",
       "      <td>0.857312</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>0.627859</td>\n",
       "      <td>0.634143</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>2.543000</td>\n",
       "      <td>668.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.901640</td>\n",
       "      <td>0.609412</td>\n",
       "      <td>0.607867</td>\n",
       "      <td>0.607085</td>\n",
       "      <td>0.609412</td>\n",
       "      <td>2.645700</td>\n",
       "      <td>642.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.517400</td>\n",
       "      <td>0.935865</td>\n",
       "      <td>0.624118</td>\n",
       "      <td>0.621405</td>\n",
       "      <td>0.623082</td>\n",
       "      <td>0.624118</td>\n",
       "      <td>2.536700</td>\n",
       "      <td>670.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.942726</td>\n",
       "      <td>0.622353</td>\n",
       "      <td>0.619813</td>\n",
       "      <td>0.621881</td>\n",
       "      <td>0.622353</td>\n",
       "      <td>2.535700</td>\n",
       "      <td>670.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>0.937869</td>\n",
       "      <td>0.622941</td>\n",
       "      <td>0.620693</td>\n",
       "      <td>0.620465</td>\n",
       "      <td>0.622941</td>\n",
       "      <td>2.534900</td>\n",
       "      <td>670.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.543900</td>\n",
       "      <td>0.972411</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>0.606726</td>\n",
       "      <td>0.608625</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>2.534800</td>\n",
       "      <td>670.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.618200</td>\n",
       "      <td>0.969854</td>\n",
       "      <td>0.628824</td>\n",
       "      <td>0.631384</td>\n",
       "      <td>0.640473</td>\n",
       "      <td>0.628824</td>\n",
       "      <td>2.616100</td>\n",
       "      <td>649.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.485200</td>\n",
       "      <td>0.996401</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.602960</td>\n",
       "      <td>0.611298</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>2.537000</td>\n",
       "      <td>670.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.995430</td>\n",
       "      <td>0.607647</td>\n",
       "      <td>0.603322</td>\n",
       "      <td>0.604572</td>\n",
       "      <td>0.607647</td>\n",
       "      <td>2.540100</td>\n",
       "      <td>669.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>1.004964</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>0.620990</td>\n",
       "      <td>0.627174</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>2.538200</td>\n",
       "      <td>669.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.410900</td>\n",
       "      <td>1.003632</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.615226</td>\n",
       "      <td>0.616406</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>2.536700</td>\n",
       "      <td>670.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.373100</td>\n",
       "      <td>0.997813</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.620465</td>\n",
       "      <td>0.623646</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>2.540500</td>\n",
       "      <td>669.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.316100</td>\n",
       "      <td>1.032668</td>\n",
       "      <td>0.617059</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.613626</td>\n",
       "      <td>0.617059</td>\n",
       "      <td>2.538900</td>\n",
       "      <td>669.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>1.045840</td>\n",
       "      <td>0.615294</td>\n",
       "      <td>0.615250</td>\n",
       "      <td>0.615215</td>\n",
       "      <td>0.615294</td>\n",
       "      <td>2.538000</td>\n",
       "      <td>669.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>1.089537</td>\n",
       "      <td>0.604118</td>\n",
       "      <td>0.596663</td>\n",
       "      <td>0.601123</td>\n",
       "      <td>0.604118</td>\n",
       "      <td>2.536700</td>\n",
       "      <td>670.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>1.078352</td>\n",
       "      <td>0.616471</td>\n",
       "      <td>0.615286</td>\n",
       "      <td>0.615298</td>\n",
       "      <td>0.616471</td>\n",
       "      <td>2.534400</td>\n",
       "      <td>670.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.354900</td>\n",
       "      <td>1.102034</td>\n",
       "      <td>0.608824</td>\n",
       "      <td>0.604882</td>\n",
       "      <td>0.606544</td>\n",
       "      <td>0.608824</td>\n",
       "      <td>2.539500</td>\n",
       "      <td>669.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>1.093933</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>0.611052</td>\n",
       "      <td>0.610877</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>2.613300</td>\n",
       "      <td>650.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>1.103181</td>\n",
       "      <td>0.618235</td>\n",
       "      <td>0.617229</td>\n",
       "      <td>0.616633</td>\n",
       "      <td>0.618235</td>\n",
       "      <td>2.532800</td>\n",
       "      <td>671.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>1.115664</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>0.616324</td>\n",
       "      <td>0.616186</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>2.532700</td>\n",
       "      <td>671.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>1.113701</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>0.612676</td>\n",
       "      <td>0.612916</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>2.535700</td>\n",
       "      <td>670.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.361300</td>\n",
       "      <td>1.109786</td>\n",
       "      <td>0.617059</td>\n",
       "      <td>0.616476</td>\n",
       "      <td>0.616589</td>\n",
       "      <td>0.617059</td>\n",
       "      <td>2.528800</td>\n",
       "      <td>672.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.336900</td>\n",
       "      <td>1.109535</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>0.618747</td>\n",
       "      <td>0.618751</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>2.534600</td>\n",
       "      <td>670.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.361900</td>\n",
       "      <td>1.110078</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.618232</td>\n",
       "      <td>0.618667</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>2.608000</td>\n",
       "      <td>651.835000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.6564594264576833, metrics={'train_runtime': 867.6076, 'train_samples_per_second': 0.692, 'total_flos': 3165964645599000, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6919655203819275,\n",
       " 'eval_accuracy': 0.6953333333333334,\n",
       " 'eval_f1': 0.6963905876614596,\n",
       " 'eval_precision': 0.6999825608500596,\n",
       " 'eval_recall': 0.6953333333333334,\n",
       " 'eval_runtime': 5.0704,\n",
       " 'eval_samples_per_second': 591.667}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('/media/usama/48E0E582E0E5769A/Users/AIM-LAB-SERVER/Desktop/results_senti_mix/best-checkpoint_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17868), started 1 day, 17:47:06 ago. (Use '!kill 17868' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}