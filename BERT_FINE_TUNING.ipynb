{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'bilingual_vocab_original'\n",
    "\n",
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint $MODEL_PATH/model.ckpt-100000 \\\n",
    "#   --config $MODEL_PATH/config.json \\\n",
    "#   --pytorch_dump_output $MODEL_PATH/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments,DataCollatorWithPadding,DataCollatorForTokenClassification,BertForQuestionAnswering,BertForSequenceClassification,BertForTokenClassification,BertTokenizerFast\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = '../glue-urdu/'\n",
    "TASK = 'POS'\n",
    "task_params = {\n",
    "    'NER':{'best_model_metric':'eval_f1','entity_metrics':True,'batch_size':32, 'epochs':3},\n",
    "    'NLI':{'batch_size':64, 'epochs':3},\n",
    "    'POS':{'best_model_metric':'eval_f1','entity_metrics':True,'batch_size':16, 'epochs':3},\n",
    "    'QuAD':{'best_model_metric':'eval_f1','batch_size':32, 'epochs':10},\n",
    "    'SentiMix':{'best_model_metric':'eval_f1','batch_size':64, 'epochs':3}\n",
    "}\n",
    "type_datasets=['train','validation','test']\n",
    "assert TASK in task_params.keys()\n",
    "hyperparams = task_params[TASK]\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if TASK == 'POS':\n",
    "            item['labels'] = self.labels[idx]\n",
    "        elif TASK != 'QuAD':\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "def answer_to_idx(answers, contexts):\n",
    "    result = []\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        start_idx = int(answer[0])\n",
    "        end_idx = start_idx + len(answer[1])\n",
    "\n",
    "        for i in range(1, 3):\n",
    "            if context[start_idx - i:end_idx - i+1] == answer[1]:\n",
    "                start_idx -= i\n",
    "                end_idx -= i\n",
    "                break\n",
    "        result.append([start_idx, end_idx, (start_idx,answer[1])])\n",
    "\n",
    "    return result\n",
    "\n",
    "def char_to_token_position(encodings, answers,dataset_type):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    answer_texts = []\n",
    "    for i,answer in enumerate(answers):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i][0]) or 128)\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i][1] - 1) or 128)\n",
    "        answer_texts.append((answer[2],encodings['input_ids'][i]))\n",
    "\n",
    "    return dict(encodings, **{'start_positions': start_positions, 'end_positions': end_positions}),answer_texts\n",
    "\n",
    "def encode_tags(tags, encodings, unique_tags):\n",
    "    labels = [[unique_tags.index(tag) for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset), dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        if sum(arr_offset[-2]) != 0:\n",
    "            sub_len = sum((arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0))\n",
    "        else:\n",
    "            sub_len = len(doc_labels)\n",
    "        doc_enc_labels[(arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)] = doc_labels[:sub_len]\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "def common_NER_POS(token_tags,num_tags):\n",
    "    unique_tags = list(set(tag for doc in token_tags[0][1] + token_tags[1][1] for tag in doc))\n",
    "    assert len(unique_tags) == num_tags\n",
    "    X_train, X_val, y_train, y_val = train_test_split(*token_tags[0], test_size=0.1)\n",
    "    X_test, y_test = token_tags[1]\n",
    "\n",
    "    encodings = []\n",
    "    labels = []\n",
    "    for data, label in [(X_train, y_train), (X_val, y_val), (X_test, y_test)]:\n",
    "        encodings.append(\n",
    "            tokenizer(list(data), is_split_into_words=True, return_offsets_mapping=True, padding='max_length',\n",
    "                      truncation=True, add_special_tokens=True, return_attention_mask=True,\n",
    "                      return_tensors=\"pt\", max_length=128))\n",
    "        labels.append(encode_tags(label, encodings[-1], unique_tags))\n",
    "        encodings[-1].pop(\"offset_mapping\")\n",
    "\n",
    "    CustomDataset.label_list = unique_tags\n",
    "    return {'train': CustomDataset(encodings[0], labels[0]),\n",
    "            'validation': CustomDataset(encodings[1], labels[1]),\n",
    "            'test': CustomDataset(encodings[2], labels[2])}\n",
    "\n",
    "def getSentiMix(path):\n",
    "    senti_mix_train = pd.read_csv(path+'SentiMix/Roman Urdu/SentiMix.train.ru.csv')\n",
    "    senti_mix_test = pd.read_csv(path+'SentiMix/Roman Urdu/SentiMix.test.ru.csv')\n",
    "    sentiment_categorical = senti_mix_train['sentiment'].astype('category').cat\n",
    "    class_names = list(sentiment_categorical.categories)\n",
    "\n",
    "    sentences_train = list(senti_mix_train.sentence)\n",
    "    labels_train = list(sentiment_categorical.codes)\n",
    "\n",
    "    X_test = list(senti_mix_test.sentence)\n",
    "    y_test = list(senti_mix_test['sentiment'].astype('category').cat.codes)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(sentences_train, labels_train, test_size=0.1)\n",
    "    encodings = []\n",
    "    for data in [X_train,X_val,X_test]:\n",
    "        encodings.append(tokenizer(data,padding='max_length', truncation=True, add_special_tokens = True, return_attention_mask = True, return_tensors = \"pt\", max_length=128))\n",
    "\n",
    "    return {'train': CustomDataset(encodings[0],y_train),\n",
    "            'validation': CustomDataset(encodings[1],y_val),\n",
    "            'test': CustomDataset(encodings[2],y_test),\n",
    "            'classes':class_names}\n",
    "\n",
    "def getNLI(path):\n",
    "    data_dict = {}\n",
    "    for i in ['train','dev','test']:\n",
    "        dataframe = pd.read_csv(path+'NLI/Roman Urdu/NLI.ru.{}.tsv'.format(i),sep='\\t')\n",
    "        sentences = dataframe[['premise','hypo']].to_numpy()\n",
    "        categorical = dataframe['Label'].astype('category').cat\n",
    "        labels = list(categorical.codes)\n",
    "        data = list(map(str.strip,sentences[:,0])),list(map(str.strip,sentences[:,1]))\n",
    "        encodings = tokenizer(*data,padding='max_length', truncation=True, add_special_tokens = True, return_attention_mask = True, return_tensors = \"pt\", max_length=128)\n",
    "        data_dict[i] = CustomDataset(encodings,labels)\n",
    "        if i == 'train':\n",
    "            data_dict['classes'] = list(categorical.categories)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def getNER(path):\n",
    "    token_tags = []\n",
    "    for split_type in ['train', 'test']:\n",
    "        raw_docs = open(path + 'NER/Roman Urdu/NER.ru.{}'.format(split_type)).read().strip().split('\\n\\n')\n",
    "        processed_docs = [list(zip(*[token_tag.split('\\t') for token_tag in doc.split('\\n')])) for doc in raw_docs]\n",
    "        token_tags.append(list(zip(*processed_docs)))\n",
    "\n",
    "    return common_NER_POS(token_tags,7)\n",
    "\n",
    "def getPOS(path):\n",
    "    token_tags = []\n",
    "    for split_type in ['train','dev' ,'test']:\n",
    "        raw_docs = open(path + 'POS/Roman Urdu/pos.ru.{}.conllu'.format(split_type)).read().strip().split('\\n\\n')\n",
    "        processed_docs = [list(zip(*[itemgetter(1,3)(token_tag.split('\\t')) for token_tag in doc.split('\\n')[2:]])) for doc in raw_docs]\n",
    "        token_tags.append(list(zip(*processed_docs)))\n",
    "\n",
    "    return common_NER_POS(token_tags,17)\n",
    "\n",
    "def getQuAD(path):\n",
    "    dataframe = pd.read_csv(path + 'QuAD/Roman Urdu/QuAD.ru.csv', sep=r\"\\s\\|\\s\", engine='python')\n",
    "    sentences = dataframe[[\"paragraph\", \"question\"]].to_numpy()\n",
    "    answers = dataframe[[\"answer starting idx\", \"answer\"]].to_numpy()\n",
    "\n",
    "    y = answer_to_idx(answers, sentences[:, 0])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125,random_state=1)  # 0.125 * 0.8 = 0.1\n",
    "\n",
    "    encodings = []\n",
    "    for data in [X_train, X_val, X_test]:\n",
    "        encodings.append(\n",
    "            tokenizer(list(data[:, 0]), list(data[:, 1]), padding='max_length', truncation='only_first', add_special_tokens=True,\n",
    "                      return_attention_mask=True,\n",
    "                      return_tensors=\"pt\", max_length=128))\n",
    "\n",
    "    return {'train': CustomDataset(*char_to_token_position(encodings[0], y_train,0)),\n",
    "            'validation': CustomDataset(*char_to_token_position(encodings[1], y_val,1)),\n",
    "            'test': CustomDataset(*char_to_token_position(encodings[2], y_test,2))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bilingual_vocab_original were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bilingual_vocab_original and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "fine_tune_dataset = locals()['get'+TASK](DATA_ROOT)\n",
    "if TASK == 'QuAD':\n",
    "    metric = load_metric(\"squad\")\n",
    "elif TASK == 'NER' or TASK == 'POS':\n",
    "    metric = load_metric(\"seqeval\")\n",
    "\n",
    "if TASK == 'QuAD':\n",
    "    model = BertForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "elif TASK == 'NER' or TASK == 'POS':\n",
    "    model = BertForTokenClassification.from_pretrained(MODEL_PATH,num_labels=len(CustomDataset.label_list))\n",
    "else:\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=3)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    if TASK == 'QuAD':\n",
    "        answer_start_ids = np.argmax(pred.predictions[0],axis=-1)\n",
    "        answer_end_ids = np.argmax(pred.predictions[1],axis=-1)\n",
    "        labels = fine_tune_dataset['validation'].labels\n",
    "        predictions=[]\n",
    "        references=[]\n",
    "        for i,_id in enumerate(range(len(labels))):\n",
    "            true_answer,encodings = labels[_id]\n",
    "            predictions.append({'id':str(_id),'prediction_text':tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encodings[answer_start_ids[i]:answer_end_ids[i]]))})\n",
    "            references.append({'id':str(_id),'answers':{'answer_start':[true_answer[0]],'text':[true_answer[1]]}})\n",
    "\n",
    "        return metric.compute(predictions=predictions, references=references)\n",
    "    elif TASK == 'NER' or TASK == 'POS':\n",
    "        predictions, labels = pred\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        true_predictions,true_labels = list(zip(*[zip(*map(lambda p_l:itemgetter(*p_l)(CustomDataset.label_list),\n",
    "                                                           filter(lambda p_l: p_l[1] != -100, zip(prediction,label))))\n",
    "                                                  for prediction, label in zip(predictions,labels)]))\n",
    "\n",
    "        results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        if hyperparams['entity_metrics']:\n",
    "            # Unpack nested dictionaries\n",
    "            final_results = {}\n",
    "            for key, value in results.items():\n",
    "                if isinstance(value, dict):\n",
    "                    for n, v in value.items():\n",
    "                        final_results[f\"{key}_{n}\"] = v\n",
    "                else:\n",
    "                    final_results[key] = value\n",
    "            return final_results\n",
    "        else:\n",
    "            return {\n",
    "                \"precision\": results[\"overall_precision\"],\n",
    "                \"recall\": results[\"overall_recall\"],\n",
    "                \"f1\": results[\"overall_f1\"],\n",
    "                \"accuracy\": results[\"overall_accuracy\"],\n",
    "            }\n",
    "    else:\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='fine_tune_results/{}'.format(TASK),  # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=hyperparams['epochs'],  # total number of training epochs\n",
    "    per_device_train_batch_size=hyperparams['batch_size'],  # batch size per device during training\n",
    "    per_device_eval_batch_size=hyperparams['batch_size'],  # batch size for evaluation\n",
    "    warmup_steps=60,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir='./logs',  # directory for storing logs\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps = 10,\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=hyperparams['best_model_metric']\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=fine_tune_dataset['train'],  # training dataset\n",
    "    eval_dataset=fine_tune_dataset['validation'],  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer) if TASK == 'NER' or TASK == 'POS' else DataCollatorWithPadding(tokenizer)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='71' max='342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 71/342 00:41 < 02:43, 1.66 it/s, Epoch 0.61/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Art Precision</th>\n",
       "      <th>Art Recall</th>\n",
       "      <th>Art F1</th>\n",
       "      <th>Art Number</th>\n",
       "      <th>Conj Precision</th>\n",
       "      <th>Conj Recall</th>\n",
       "      <th>Conj F1</th>\n",
       "      <th>Conj Number</th>\n",
       "      <th>Dj Precision</th>\n",
       "      <th>Dj Recall</th>\n",
       "      <th>Dj F1</th>\n",
       "      <th>Dj Number</th>\n",
       "      <th>Dp Precision</th>\n",
       "      <th>Dp Recall</th>\n",
       "      <th>Dp F1</th>\n",
       "      <th>Dp Number</th>\n",
       "      <th>Dv Precision</th>\n",
       "      <th>Dv Recall</th>\n",
       "      <th>Dv F1</th>\n",
       "      <th>Dv Number</th>\n",
       "      <th>Erb Precision</th>\n",
       "      <th>Erb Recall</th>\n",
       "      <th>Erb F1</th>\n",
       "      <th>Erb Number</th>\n",
       "      <th>Et Precision</th>\n",
       "      <th>Et Recall</th>\n",
       "      <th>Et F1</th>\n",
       "      <th>Et Number</th>\n",
       "      <th>Ntj Precision</th>\n",
       "      <th>Ntj Recall</th>\n",
       "      <th>Ntj F1</th>\n",
       "      <th>Ntj Number</th>\n",
       "      <th>Oun Precision</th>\n",
       "      <th>Oun Recall</th>\n",
       "      <th>Oun F1</th>\n",
       "      <th>Oun Number</th>\n",
       "      <th>Ron Precision</th>\n",
       "      <th>Ron Recall</th>\n",
       "      <th>Ron F1</th>\n",
       "      <th>Ron Number</th>\n",
       "      <th>Ropn Precision</th>\n",
       "      <th>Ropn Recall</th>\n",
       "      <th>Ropn F1</th>\n",
       "      <th>Ropn Number</th>\n",
       "      <th>Um Precision</th>\n",
       "      <th>Um Recall</th>\n",
       "      <th>Um F1</th>\n",
       "      <th>Um Number</th>\n",
       "      <th>Unct Precision</th>\n",
       "      <th>Unct Recall</th>\n",
       "      <th>Unct F1</th>\n",
       "      <th>Unct Number</th>\n",
       "      <th>Ux Precision</th>\n",
       "      <th>Ux Recall</th>\n",
       "      <th>Ux F1</th>\n",
       "      <th>Ux Number</th>\n",
       "      <th>Ym Precision</th>\n",
       "      <th>Ym Recall</th>\n",
       "      <th>Ym F1</th>\n",
       "      <th>Ym Number</th>\n",
       "      <th>  Precision</th>\n",
       "      <th>  Recall</th>\n",
       "      <th>  F1</th>\n",
       "      <th>  Number</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.695200</td>\n",
       "      <td>2.631105</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.015848</td>\n",
       "      <td>221</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.026866</td>\n",
       "      <td>456</td>\n",
       "      <td>0.204380</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.062710</td>\n",
       "      <td>756</td>\n",
       "      <td>0.197195</td>\n",
       "      <td>0.253044</td>\n",
       "      <td>0.221655</td>\n",
       "      <td>1889</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>96</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.129828</td>\n",
       "      <td>0.142186</td>\n",
       "      <td>932</td>\n",
       "      <td>0.020270</td>\n",
       "      <td>0.064171</td>\n",
       "      <td>0.030809</td>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.238733</td>\n",
       "      <td>0.184557</td>\n",
       "      <td>0.208178</td>\n",
       "      <td>2124</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.007547</td>\n",
       "      <td>407</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>0.050432</td>\n",
       "      <td>0.028478</td>\n",
       "      <td>694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.130845</td>\n",
       "      <td>0.120120</td>\n",
       "      <td>0.125253</td>\n",
       "      <td>0.235272</td>\n",
       "      <td>2.312400</td>\n",
       "      <td>175.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.142900</td>\n",
       "      <td>2.043103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>756</td>\n",
       "      <td>0.372580</td>\n",
       "      <td>0.621493</td>\n",
       "      <td>0.465873</td>\n",
       "      <td>1889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96</td>\n",
       "      <td>0.453846</td>\n",
       "      <td>0.063305</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.247326</td>\n",
       "      <td>0.359228</td>\n",
       "      <td>0.292955</td>\n",
       "      <td>2124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>407</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.311302</td>\n",
       "      <td>0.221495</td>\n",
       "      <td>0.258830</td>\n",
       "      <td>0.403761</td>\n",
       "      <td>2.298400</td>\n",
       "      <td>176.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.451600</td>\n",
       "      <td>1.303161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.038710</td>\n",
       "      <td>456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>756</td>\n",
       "      <td>0.588283</td>\n",
       "      <td>0.776072</td>\n",
       "      <td>0.669254</td>\n",
       "      <td>1889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96</td>\n",
       "      <td>0.533898</td>\n",
       "      <td>0.540773</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.404339</td>\n",
       "      <td>0.482580</td>\n",
       "      <td>0.440009</td>\n",
       "      <td>2124</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.237069</td>\n",
       "      <td>407</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.302594</td>\n",
       "      <td>0.249406</td>\n",
       "      <td>694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>158</td>\n",
       "      <td>0.995074</td>\n",
       "      <td>0.781431</td>\n",
       "      <td>0.875406</td>\n",
       "      <td>517</td>\n",
       "      <td>0.611632</td>\n",
       "      <td>0.584229</td>\n",
       "      <td>0.597617</td>\n",
       "      <td>558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.501316</td>\n",
       "      <td>0.443545</td>\n",
       "      <td>0.470664</td>\n",
       "      <td>0.625880</td>\n",
       "      <td>2.396600</td>\n",
       "      <td>168.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.798071</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.389140</td>\n",
       "      <td>0.556634</td>\n",
       "      <td>221</td>\n",
       "      <td>0.970430</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.871981</td>\n",
       "      <td>456</td>\n",
       "      <td>0.437788</td>\n",
       "      <td>0.125661</td>\n",
       "      <td>0.195272</td>\n",
       "      <td>756</td>\n",
       "      <td>0.840630</td>\n",
       "      <td>0.904711</td>\n",
       "      <td>0.871494</td>\n",
       "      <td>1889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96</td>\n",
       "      <td>0.832753</td>\n",
       "      <td>0.769313</td>\n",
       "      <td>0.799777</td>\n",
       "      <td>932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.528685</td>\n",
       "      <td>0.564030</td>\n",
       "      <td>0.545786</td>\n",
       "      <td>2124</td>\n",
       "      <td>0.577731</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.622877</td>\n",
       "      <td>407</td>\n",
       "      <td>0.314911</td>\n",
       "      <td>0.435159</td>\n",
       "      <td>0.365396</td>\n",
       "      <td>694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>158</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>0.951644</td>\n",
       "      <td>0.948891</td>\n",
       "      <td>517</td>\n",
       "      <td>0.799672</td>\n",
       "      <td>0.872760</td>\n",
       "      <td>0.834619</td>\n",
       "      <td>558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.681109</td>\n",
       "      <td>0.634650</td>\n",
       "      <td>0.657059</td>\n",
       "      <td>0.750093</td>\n",
       "      <td>2.310400</td>\n",
       "      <td>175.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.580439</td>\n",
       "      <td>0.964029</td>\n",
       "      <td>0.606335</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>221</td>\n",
       "      <td>0.953162</td>\n",
       "      <td>0.892544</td>\n",
       "      <td>0.921857</td>\n",
       "      <td>456</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.427249</td>\n",
       "      <td>0.467777</td>\n",
       "      <td>756</td>\n",
       "      <td>0.931759</td>\n",
       "      <td>0.939651</td>\n",
       "      <td>0.935688</td>\n",
       "      <td>1889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96</td>\n",
       "      <td>0.902838</td>\n",
       "      <td>0.887339</td>\n",
       "      <td>0.895022</td>\n",
       "      <td>932</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.607670</td>\n",
       "      <td>0.678908</td>\n",
       "      <td>0.641316</td>\n",
       "      <td>2124</td>\n",
       "      <td>0.636197</td>\n",
       "      <td>0.855037</td>\n",
       "      <td>0.729560</td>\n",
       "      <td>407</td>\n",
       "      <td>0.475269</td>\n",
       "      <td>0.318444</td>\n",
       "      <td>0.381363</td>\n",
       "      <td>694</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.310127</td>\n",
       "      <td>0.455814</td>\n",
       "      <td>158</td>\n",
       "      <td>0.984645</td>\n",
       "      <td>0.992263</td>\n",
       "      <td>0.988439</td>\n",
       "      <td>517</td>\n",
       "      <td>0.887348</td>\n",
       "      <td>0.917563</td>\n",
       "      <td>0.902203</td>\n",
       "      <td>558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.765519</td>\n",
       "      <td>0.737245</td>\n",
       "      <td>0.751116</td>\n",
       "      <td>0.813264</td>\n",
       "      <td>2.324300</td>\n",
       "      <td>174.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.542300</td>\n",
       "      <td>0.457513</td>\n",
       "      <td>0.954248</td>\n",
       "      <td>0.660633</td>\n",
       "      <td>0.780749</td>\n",
       "      <td>221</td>\n",
       "      <td>0.949045</td>\n",
       "      <td>0.980263</td>\n",
       "      <td>0.964401</td>\n",
       "      <td>456</td>\n",
       "      <td>0.544226</td>\n",
       "      <td>0.585979</td>\n",
       "      <td>0.564331</td>\n",
       "      <td>756</td>\n",
       "      <td>0.953183</td>\n",
       "      <td>0.959238</td>\n",
       "      <td>0.956201</td>\n",
       "      <td>1889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96</td>\n",
       "      <td>0.940331</td>\n",
       "      <td>0.913090</td>\n",
       "      <td>0.926511</td>\n",
       "      <td>932</td>\n",
       "      <td>0.848276</td>\n",
       "      <td>0.657754</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.744544</td>\n",
       "      <td>0.706685</td>\n",
       "      <td>0.725121</td>\n",
       "      <td>2124</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.884521</td>\n",
       "      <td>0.850059</td>\n",
       "      <td>407</td>\n",
       "      <td>0.484670</td>\n",
       "      <td>0.592219</td>\n",
       "      <td>0.533074</td>\n",
       "      <td>694</td>\n",
       "      <td>0.809160</td>\n",
       "      <td>0.670886</td>\n",
       "      <td>0.733564</td>\n",
       "      <td>158</td>\n",
       "      <td>0.967864</td>\n",
       "      <td>0.990329</td>\n",
       "      <td>0.978967</td>\n",
       "      <td>517</td>\n",
       "      <td>0.905498</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.924561</td>\n",
       "      <td>558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.810185</td>\n",
       "      <td>0.802906</td>\n",
       "      <td>0.806529</td>\n",
       "      <td>0.855409</td>\n",
       "      <td>2.311900</td>\n",
       "      <td>175.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='10' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/13 00:01 < 00:00, 6.27 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-3435b262f1ae>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001B[0m\n\u001B[1;32m   1170\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallback_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_step_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1171\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1172\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_log_save_evaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtr_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrial\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1173\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1174\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_epoch_stop\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_training_stop\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36m_maybe_log_save_evaluate\u001B[0;34m(self, tr_loss, model, trial, epoch)\u001B[0m\n\u001B[1;32m   1263\u001B[0m         \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1264\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_evaluate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1265\u001B[0;31m             \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1266\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_report_to_hp_search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1267\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mevaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   1762\u001B[0m             \u001B[0mprediction_loss_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_metrics\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1763\u001B[0m             \u001B[0mignore_keys\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mignore_keys\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1764\u001B[0;31m             \u001B[0mmetric_key_prefix\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmetric_key_prefix\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1765\u001B[0m         )\n\u001B[1;32m   1766\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mprediction_loop\u001B[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   1893\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1894\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataloader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1895\u001B[0;31m             \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogits\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprediction_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprediction_loss_only\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_keys\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mignore_keys\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1896\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mloss\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1897\u001B[0m                 \u001B[0mlosses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrepeat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mprediction_step\u001B[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001B[0m\n\u001B[1;32m   2029\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2030\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mhas_labels\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2031\u001B[0;31m                     \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2032\u001B[0m                     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2033\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mcompute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   1554\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1555\u001B[0m             \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1556\u001B[0;31m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1557\u001B[0m         \u001B[0;31m# Save past state if it exists\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1558\u001B[0m         \u001B[0;31m# TODO: this needs to be fixed and made cleaner later.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    165\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    166\u001B[0m         \u001B[0mreplicas\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreplicate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice_ids\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 167\u001B[0;31m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallel_apply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreplicas\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    168\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgather\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput_device\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    169\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001B[0m in \u001B[0;36mparallel_apply\u001B[0;34m(self, replicas, inputs, kwargs)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mparallel_apply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreplicas\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 177\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mparallel_apply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreplicas\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice_ids\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreplicas\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    178\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mgather\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_device\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\u001B[0m in \u001B[0;36mparallel_apply\u001B[0;34m(modules, inputs, kwargs_tup, devices)\u001B[0m\n\u001B[1;32m     76\u001B[0m             \u001B[0mthread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mthread\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mthreads\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 78\u001B[0;31m             \u001B[0mthread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     79\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     80\u001B[0m         \u001B[0m_worker\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodules\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs_tup\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevices\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.6/threading.py\u001B[0m in \u001B[0;36mjoin\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1054\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1055\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1056\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wait_for_tstate_lock\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1057\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1058\u001B[0m             \u001B[0;31m# the behavior of a negative timeout isn't documented, but\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.6/threading.py\u001B[0m in \u001B[0;36m_wait_for_tstate_lock\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m   1070\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlock\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# already determined that the C code is done\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1071\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_stopped\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1072\u001B[0;31m         \u001B[0;32melif\u001B[0m \u001B[0mlock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mblock\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1073\u001B[0m             \u001B[0mlock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelease\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1074\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(fine_tune_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('fine_tune_results/{}/best-checkpoint_f1'.format(TASK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}