{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT_BASE_DIR='bert_roman_urdu'\n",
    "\n",
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint $BERT_BASE_DIR/model.ckpt-100000 \\\n",
    "#   --config $BERT_BASE_DIR/config.json \\\n",
    "#   --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification,BertForQuestionAnswering, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file bert-base-uncased/config.json not found\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'bert-base-uncased'. Make sure that:\n\n- 'bert-base-uncased' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'bert-base-uncased' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m~/Desktop/PycharmProjects/FAST/AIM_LAB/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    423\u001B[0m                 \u001B[0mlocal_files_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlocal_files_only\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 424\u001B[0;31m                 \u001B[0muse_auth_token\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    425\u001B[0m             )\n",
      "\u001B[0;32m~/Desktop/PycharmProjects/FAST/AIM_LAB/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m   1092\u001B[0m         \u001B[0;31m# File, but it doesn't exist.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1093\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"file {} not found\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl_or_filename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1094\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: file bert-base-uncased/config.json not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-8bbcf600f4bc>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mmodel_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'bert-base-uncased'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mTASK\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'QuAD'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBertForQuestionAnswering\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBertForSequenceClassification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/PycharmProjects/FAST/AIM_LAB/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/modeling_utils.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    960\u001B[0m                 \u001B[0muse_auth_token\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    961\u001B[0m                 \u001B[0mrevision\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrevision\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 962\u001B[0;31m                 \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    963\u001B[0m             )\n\u001B[1;32m    964\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/PycharmProjects/FAST/AIM_LAB/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    375\u001B[0m         \"\"\"\n\u001B[0;32m--> 376\u001B[0;31m         \u001B[0mconfig_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_config_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    377\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/PycharmProjects/FAST/AIM_LAB/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    434\u001B[0m                 \u001B[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    435\u001B[0m             )\n\u001B[0;32m--> 436\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    437\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    438\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mJSONDecodeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: Can't load config for 'bert-base-uncased'. Make sure that:\n\n- 'bert-base-uncased' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'bert-base-uncased' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = '../glue-urdu/'\n",
    "TASK = 'QuAD'\n",
    "assert TASK in ['NER','NLI','POS','QuAD','SentiMix']\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_path = 'bert-base-uncased'\n",
    "if TASK == 'QuAD':\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "else:\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "def answer_to_idx(answers, contexts):\n",
    "    result = []\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        start_idx = int(answer[0])\n",
    "        end_idx = start_idx + len(answer[1])\n",
    "\n",
    "        for i in range(1, 3):\n",
    "            if context[start_idx - i:end_idx - i] == answer[1]:\n",
    "                start_idx -= i\n",
    "                end_idx -= i\n",
    "                break\n",
    "        result.append([start_idx, end_idx])\n",
    "\n",
    "    return result\n",
    "\n",
    "def char_to_token_position(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i][0]) or 128)\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i][1] - 1) or 128)\n",
    "\n",
    "    return dict(encodings, **{'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "\n",
    "def getSentiMix(path):\n",
    "    senti_mix_train = pd.read_csv(path+'SentiMix/Roman Urdu/SentiMix.train.ru.csv')\n",
    "    senti_mix_test = pd.read_csv(path+'SentiMix/Roman Urdu/SentiMix.test.ru.csv')\n",
    "    sentiment_categorical = senti_mix_train['sentiment'].astype('category').cat\n",
    "    class_names = list(sentiment_categorical.categories)\n",
    "\n",
    "    sentences_train = list(senti_mix_train.sentence)\n",
    "    labels_train = list(sentiment_categorical.codes)\n",
    "\n",
    "    X_test = list(senti_mix_test.sentence)\n",
    "    y_test = list(senti_mix_test['sentiment'].astype('category').cat.codes)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(sentences_train, labels_train, test_size=0.1)\n",
    "    encodings = []\n",
    "    for data in [X_train,X_val,X_test]:\n",
    "        encodings.append(tokenizer(data,padding='max_length', truncation=True, add_special_tokens = True, return_attention_mask = True, return_tensors = \"pt\", max_length=128))\n",
    "\n",
    "    return {'train': CustomDataset(encodings[0],y_train),\n",
    "            'validation': CustomDataset(encodings[1],y_val),\n",
    "            'test': CustomDataset(encodings[2],y_test),\n",
    "            'classes':class_names}\n",
    "\n",
    "def getNLI(path):\n",
    "    data_dict = {}\n",
    "    for i in ['train','dev','test']:\n",
    "        dataframe = pd.read_csv(path+'NLI/Roman Urdu/NLI.ru.{}.tsv'.format(i),sep='\\t')\n",
    "        sentences = dataframe[['premise','hypo']].to_numpy()\n",
    "        categorical = dataframe['Label'].astype('category').cat\n",
    "        labels = list(categorical.codes)\n",
    "        data = list(map(str.strip,sentences[:,0])),list(map(str.strip,sentences[:,1]))\n",
    "        encodings = tokenizer(*data,padding='max_length', truncation=True, add_special_tokens = True, return_attention_mask = True, return_tensors = \"pt\", max_length=128)\n",
    "        data_dict[i] = CustomDataset(encodings,labels)\n",
    "        if i == 'train':\n",
    "            data_dict['classes'] = list(categorical.categories)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def getNER(path):\n",
    "    return\n",
    "\n",
    "def getPOS(path):\n",
    "    return\n",
    "\n",
    "def getQuAD(path):\n",
    "    dataframe = pd.read_csv(path + 'QuAD/Roman Urdu/QuAD.ru.csv', sep=r\"\\s\\|\\s\", engine='python')\n",
    "    sentences = dataframe[[\"paragraph\", \"question\"]].to_numpy()\n",
    "    answers = dataframe[[\"answer starting idx\", \"answer\"]].to_numpy()\n",
    "\n",
    "    y = answer_to_idx(answers, sentences[:, 0])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125,random_state=1)  # 0.125 * 0.8 = 0.1\n",
    "\n",
    "    encodings = []\n",
    "    for data in [X_train, X_val, X_test]:\n",
    "        encodings.append(\n",
    "            tokenizer(list(data[:, 0]), list(data[:, 1]), padding='max_length', truncation='only_first', add_special_tokens=True,\n",
    "                      return_attention_mask=True,\n",
    "                      return_tensors=\"pt\", max_length=128))\n",
    "\n",
    "    return {'train': CustomDataset(char_to_token_position(encodings[0], y_train)),\n",
    "            'validation': CustomDataset(char_to_token_position(encodings[1], y_val)),\n",
    "            'test': CustomDataset(char_to_token_position(encodings[2], y_test))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fine_tune_dataset = locals()['get'+TASK](DATA_ROOT)\n",
    "squad_metric = load_metric(\"squad\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    if TASK == 'QuAD':\n",
    "        print(pred.predictions[:3])\n",
    "        print(pred.label_ids[:3])\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='fine_tune_results/{}'.format(TASK),  # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # total number of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    warmup_steps=60,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir='./logs',  # directory for storing logs\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=EvaluationStrategy.STEPS,\n",
    "    eval_steps = 10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=fine_tune_dataset['train'],  # training dataset\n",
    "    eval_dataset=fine_tune_dataset['validation'],  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 14:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.137700</td>\n",
       "      <td>1.103790</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.318199</td>\n",
       "      <td>0.364762</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>2.493200</td>\n",
       "      <td>681.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.073600</td>\n",
       "      <td>1.070576</td>\n",
       "      <td>0.414706</td>\n",
       "      <td>0.384199</td>\n",
       "      <td>0.424678</td>\n",
       "      <td>0.414706</td>\n",
       "      <td>2.520100</td>\n",
       "      <td>674.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.045300</td>\n",
       "      <td>1.031316</td>\n",
       "      <td>0.474706</td>\n",
       "      <td>0.463536</td>\n",
       "      <td>0.511092</td>\n",
       "      <td>0.474706</td>\n",
       "      <td>2.524600</td>\n",
       "      <td>673.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.996700</td>\n",
       "      <td>0.986562</td>\n",
       "      <td>0.501765</td>\n",
       "      <td>0.502409</td>\n",
       "      <td>0.507116</td>\n",
       "      <td>0.501765</td>\n",
       "      <td>2.515800</td>\n",
       "      <td>675.735000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.937300</td>\n",
       "      <td>0.949216</td>\n",
       "      <td>0.541765</td>\n",
       "      <td>0.540090</td>\n",
       "      <td>0.548061</td>\n",
       "      <td>0.541765</td>\n",
       "      <td>2.509000</td>\n",
       "      <td>677.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.962700</td>\n",
       "      <td>0.909383</td>\n",
       "      <td>0.554118</td>\n",
       "      <td>0.551958</td>\n",
       "      <td>0.553031</td>\n",
       "      <td>0.554118</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>677.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.992800</td>\n",
       "      <td>0.885858</td>\n",
       "      <td>0.562353</td>\n",
       "      <td>0.549311</td>\n",
       "      <td>0.562537</td>\n",
       "      <td>0.562353</td>\n",
       "      <td>2.510200</td>\n",
       "      <td>677.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.860500</td>\n",
       "      <td>0.857858</td>\n",
       "      <td>0.580588</td>\n",
       "      <td>0.577580</td>\n",
       "      <td>0.581693</td>\n",
       "      <td>0.580588</td>\n",
       "      <td>2.529000</td>\n",
       "      <td>672.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.842583</td>\n",
       "      <td>0.598824</td>\n",
       "      <td>0.595259</td>\n",
       "      <td>0.594230</td>\n",
       "      <td>0.598824</td>\n",
       "      <td>2.527900</td>\n",
       "      <td>672.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.945100</td>\n",
       "      <td>0.832873</td>\n",
       "      <td>0.604706</td>\n",
       "      <td>0.601682</td>\n",
       "      <td>0.601114</td>\n",
       "      <td>0.604706</td>\n",
       "      <td>2.530900</td>\n",
       "      <td>671.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.877800</td>\n",
       "      <td>0.845043</td>\n",
       "      <td>0.598235</td>\n",
       "      <td>0.588168</td>\n",
       "      <td>0.600308</td>\n",
       "      <td>0.598235</td>\n",
       "      <td>2.532700</td>\n",
       "      <td>671.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.839700</td>\n",
       "      <td>0.819546</td>\n",
       "      <td>0.613529</td>\n",
       "      <td>0.611139</td>\n",
       "      <td>0.613560</td>\n",
       "      <td>0.613529</td>\n",
       "      <td>2.531900</td>\n",
       "      <td>671.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.781500</td>\n",
       "      <td>0.819191</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>0.611743</td>\n",
       "      <td>0.610910</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>2.535600</td>\n",
       "      <td>670.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.813100</td>\n",
       "      <td>0.820773</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.621026</td>\n",
       "      <td>0.625045</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>2.535900</td>\n",
       "      <td>670.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.786800</td>\n",
       "      <td>0.826658</td>\n",
       "      <td>0.618235</td>\n",
       "      <td>0.616280</td>\n",
       "      <td>0.615959</td>\n",
       "      <td>0.618235</td>\n",
       "      <td>2.534000</td>\n",
       "      <td>670.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.796700</td>\n",
       "      <td>0.824105</td>\n",
       "      <td>0.627647</td>\n",
       "      <td>0.625130</td>\n",
       "      <td>0.626192</td>\n",
       "      <td>0.627647</td>\n",
       "      <td>2.613300</td>\n",
       "      <td>650.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.708100</td>\n",
       "      <td>0.846899</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>0.585669</td>\n",
       "      <td>0.606449</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>2.553000</td>\n",
       "      <td>665.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.902100</td>\n",
       "      <td>0.811676</td>\n",
       "      <td>0.620588</td>\n",
       "      <td>0.623105</td>\n",
       "      <td>0.639104</td>\n",
       "      <td>0.620588</td>\n",
       "      <td>2.662600</td>\n",
       "      <td>638.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.815135</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.632816</td>\n",
       "      <td>0.650698</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>2.800900</td>\n",
       "      <td>606.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.808800</td>\n",
       "      <td>0.809087</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>0.625333</td>\n",
       "      <td>0.626181</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>2.619500</td>\n",
       "      <td>648.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.884400</td>\n",
       "      <td>0.809077</td>\n",
       "      <td>0.633529</td>\n",
       "      <td>0.631522</td>\n",
       "      <td>0.632305</td>\n",
       "      <td>0.633529</td>\n",
       "      <td>2.670000</td>\n",
       "      <td>636.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.827697</td>\n",
       "      <td>0.620588</td>\n",
       "      <td>0.618695</td>\n",
       "      <td>0.622010</td>\n",
       "      <td>0.620588</td>\n",
       "      <td>2.584200</td>\n",
       "      <td>657.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.816800</td>\n",
       "      <td>0.799385</td>\n",
       "      <td>0.642941</td>\n",
       "      <td>0.645052</td>\n",
       "      <td>0.650374</td>\n",
       "      <td>0.642941</td>\n",
       "      <td>2.547900</td>\n",
       "      <td>667.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.797952</td>\n",
       "      <td>0.631765</td>\n",
       "      <td>0.625549</td>\n",
       "      <td>0.626384</td>\n",
       "      <td>0.631765</td>\n",
       "      <td>2.647600</td>\n",
       "      <td>642.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.685300</td>\n",
       "      <td>0.822782</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.619465</td>\n",
       "      <td>0.621253</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>2.541900</td>\n",
       "      <td>668.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.783600</td>\n",
       "      <td>0.851735</td>\n",
       "      <td>0.622353</td>\n",
       "      <td>0.624137</td>\n",
       "      <td>0.636669</td>\n",
       "      <td>0.622353</td>\n",
       "      <td>2.571600</td>\n",
       "      <td>661.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.677500</td>\n",
       "      <td>0.843812</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>0.625868</td>\n",
       "      <td>0.626092</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>2.641000</td>\n",
       "      <td>643.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>0.844206</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.628122</td>\n",
       "      <td>0.631844</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>2.586300</td>\n",
       "      <td>657.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0.833472</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>0.628622</td>\n",
       "      <td>0.632889</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>2.553500</td>\n",
       "      <td>665.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.729600</td>\n",
       "      <td>0.830937</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>0.628780</td>\n",
       "      <td>0.633703</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>2.548500</td>\n",
       "      <td>667.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.838753</td>\n",
       "      <td>0.635882</td>\n",
       "      <td>0.637371</td>\n",
       "      <td>0.643270</td>\n",
       "      <td>0.635882</td>\n",
       "      <td>2.541900</td>\n",
       "      <td>668.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.857858</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>0.607769</td>\n",
       "      <td>0.609485</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>2.617300</td>\n",
       "      <td>649.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.851499</td>\n",
       "      <td>0.612353</td>\n",
       "      <td>0.605327</td>\n",
       "      <td>0.609556</td>\n",
       "      <td>0.612353</td>\n",
       "      <td>2.539700</td>\n",
       "      <td>669.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.643300</td>\n",
       "      <td>0.843805</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.621257</td>\n",
       "      <td>0.620465</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>2.540600</td>\n",
       "      <td>669.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>0.831192</td>\n",
       "      <td>0.624118</td>\n",
       "      <td>0.623818</td>\n",
       "      <td>0.624241</td>\n",
       "      <td>0.624118</td>\n",
       "      <td>2.539600</td>\n",
       "      <td>669.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.806800</td>\n",
       "      <td>0.859696</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>0.609561</td>\n",
       "      <td>0.617097</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>2.542100</td>\n",
       "      <td>668.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.639400</td>\n",
       "      <td>0.857312</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>0.627859</td>\n",
       "      <td>0.634143</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>2.543000</td>\n",
       "      <td>668.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.901640</td>\n",
       "      <td>0.609412</td>\n",
       "      <td>0.607867</td>\n",
       "      <td>0.607085</td>\n",
       "      <td>0.609412</td>\n",
       "      <td>2.645700</td>\n",
       "      <td>642.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.517400</td>\n",
       "      <td>0.935865</td>\n",
       "      <td>0.624118</td>\n",
       "      <td>0.621405</td>\n",
       "      <td>0.623082</td>\n",
       "      <td>0.624118</td>\n",
       "      <td>2.536700</td>\n",
       "      <td>670.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.942726</td>\n",
       "      <td>0.622353</td>\n",
       "      <td>0.619813</td>\n",
       "      <td>0.621881</td>\n",
       "      <td>0.622353</td>\n",
       "      <td>2.535700</td>\n",
       "      <td>670.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>0.937869</td>\n",
       "      <td>0.622941</td>\n",
       "      <td>0.620693</td>\n",
       "      <td>0.620465</td>\n",
       "      <td>0.622941</td>\n",
       "      <td>2.534900</td>\n",
       "      <td>670.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.543900</td>\n",
       "      <td>0.972411</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>0.606726</td>\n",
       "      <td>0.608625</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>2.534800</td>\n",
       "      <td>670.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.618200</td>\n",
       "      <td>0.969854</td>\n",
       "      <td>0.628824</td>\n",
       "      <td>0.631384</td>\n",
       "      <td>0.640473</td>\n",
       "      <td>0.628824</td>\n",
       "      <td>2.616100</td>\n",
       "      <td>649.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.485200</td>\n",
       "      <td>0.996401</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.602960</td>\n",
       "      <td>0.611298</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>2.537000</td>\n",
       "      <td>670.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.995430</td>\n",
       "      <td>0.607647</td>\n",
       "      <td>0.603322</td>\n",
       "      <td>0.604572</td>\n",
       "      <td>0.607647</td>\n",
       "      <td>2.540100</td>\n",
       "      <td>669.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>1.004964</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>0.620990</td>\n",
       "      <td>0.627174</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>2.538200</td>\n",
       "      <td>669.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.410900</td>\n",
       "      <td>1.003632</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.615226</td>\n",
       "      <td>0.616406</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>2.536700</td>\n",
       "      <td>670.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.373100</td>\n",
       "      <td>0.997813</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.620465</td>\n",
       "      <td>0.623646</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>2.540500</td>\n",
       "      <td>669.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.316100</td>\n",
       "      <td>1.032668</td>\n",
       "      <td>0.617059</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.613626</td>\n",
       "      <td>0.617059</td>\n",
       "      <td>2.538900</td>\n",
       "      <td>669.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>1.045840</td>\n",
       "      <td>0.615294</td>\n",
       "      <td>0.615250</td>\n",
       "      <td>0.615215</td>\n",
       "      <td>0.615294</td>\n",
       "      <td>2.538000</td>\n",
       "      <td>669.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>1.089537</td>\n",
       "      <td>0.604118</td>\n",
       "      <td>0.596663</td>\n",
       "      <td>0.601123</td>\n",
       "      <td>0.604118</td>\n",
       "      <td>2.536700</td>\n",
       "      <td>670.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>1.078352</td>\n",
       "      <td>0.616471</td>\n",
       "      <td>0.615286</td>\n",
       "      <td>0.615298</td>\n",
       "      <td>0.616471</td>\n",
       "      <td>2.534400</td>\n",
       "      <td>670.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.354900</td>\n",
       "      <td>1.102034</td>\n",
       "      <td>0.608824</td>\n",
       "      <td>0.604882</td>\n",
       "      <td>0.606544</td>\n",
       "      <td>0.608824</td>\n",
       "      <td>2.539500</td>\n",
       "      <td>669.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>1.093933</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>0.611052</td>\n",
       "      <td>0.610877</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>2.613300</td>\n",
       "      <td>650.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>1.103181</td>\n",
       "      <td>0.618235</td>\n",
       "      <td>0.617229</td>\n",
       "      <td>0.616633</td>\n",
       "      <td>0.618235</td>\n",
       "      <td>2.532800</td>\n",
       "      <td>671.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>1.115664</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>0.616324</td>\n",
       "      <td>0.616186</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>2.532700</td>\n",
       "      <td>671.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>1.113701</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>0.612676</td>\n",
       "      <td>0.612916</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>2.535700</td>\n",
       "      <td>670.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.361300</td>\n",
       "      <td>1.109786</td>\n",
       "      <td>0.617059</td>\n",
       "      <td>0.616476</td>\n",
       "      <td>0.616589</td>\n",
       "      <td>0.617059</td>\n",
       "      <td>2.528800</td>\n",
       "      <td>672.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.336900</td>\n",
       "      <td>1.109535</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>0.618747</td>\n",
       "      <td>0.618751</td>\n",
       "      <td>0.619412</td>\n",
       "      <td>2.534600</td>\n",
       "      <td>670.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.361900</td>\n",
       "      <td>1.110078</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.618232</td>\n",
       "      <td>0.618667</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>2.608000</td>\n",
       "      <td>651.835000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.6564594264576833, metrics={'train_runtime': 867.6076, 'train_samples_per_second': 0.692, 'total_flos': 3165964645599000, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6919655203819275,\n",
       " 'eval_accuracy': 0.6953333333333334,\n",
       " 'eval_f1': 0.6963905876614596,\n",
       " 'eval_precision': 0.6999825608500596,\n",
       " 'eval_recall': 0.6953333333333334,\n",
       " 'eval_runtime': 5.0704,\n",
       " 'eval_samples_per_second': 591.667}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(fine_tune_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('fine_tune_results/{}/best-checkpoint_f1'.format(TASK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17868), started 1 day, 17:47:06 ago. (Use '!kill 17868' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}