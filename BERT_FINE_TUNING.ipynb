{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT_BASE_DIR='bert_roman_urdu'\n",
    "\n",
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint $BERT_BASE_DIR/model.ckpt-100000 \\\n",
    "#   --config $BERT_BASE_DIR/config.json \\\n",
    "#   --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification,BertForQuestionAnswering, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at multilingual_vocab_augmentation_400000_steps were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at multilingual_vocab_augmentation_400000_steps and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = '../glue-urdu/'\n",
    "TASK = 'QuAD'\n",
    "task_params = {\n",
    "    'NER':[],\n",
    "    'NLI':[],\n",
    "    'POS':[],\n",
    "    'QuAD':{'batch_size':32, 'epochs':3},\n",
    "    'SentiMix':{'batch_size':64, 'epochs':3}\n",
    "}\n",
    "assert TASK in task_params.keys()\n",
    "hyperparams = task_params[TASK]\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_path = 'multilingual_vocab_augmentation_400000_steps'\n",
    "if TASK == 'QuAD':\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "else:\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        else: item['labels'] = self\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "def answer_to_idx(answers, contexts):\n",
    "    result = []\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        start_idx = int(answer[0])\n",
    "        end_idx = start_idx + len(answer[1])\n",
    "\n",
    "        for i in range(1, 3):\n",
    "            if context[start_idx - i:end_idx - i] == answer[1]:\n",
    "                start_idx -= i\n",
    "                end_idx -= i\n",
    "                break\n",
    "        result.append([start_idx, end_idx])\n",
    "\n",
    "    return result\n",
    "\n",
    "def char_to_token_position(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i][0]) or 128)\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i][1] - 1) or 128)\n",
    "\n",
    "    return dict(encodings, **{'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "\n",
    "def getSentiMix(path):\n",
    "    senti_mix_train = pd.read_csv(path+'SentiMix/Roman Urdu/SentiMix.train.ru.csv')\n",
    "    senti_mix_test = pd.read_csv(path+'SentiMix/Roman Urdu/SentiMix.test.ru.csv')\n",
    "    sentiment_categorical = senti_mix_train['sentiment'].astype('category').cat\n",
    "    class_names = list(sentiment_categorical.categories)\n",
    "\n",
    "    sentences_train = list(senti_mix_train.sentence)\n",
    "    labels_train = list(sentiment_categorical.codes)\n",
    "\n",
    "    X_test = list(senti_mix_test.sentence)\n",
    "    y_test = list(senti_mix_test['sentiment'].astype('category').cat.codes)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(sentences_train, labels_train, test_size=0.1)\n",
    "    encodings = []\n",
    "    for data in [X_train,X_val,X_test]:\n",
    "        encodings.append(tokenizer(data,padding='max_length', truncation=True, add_special_tokens = True, return_attention_mask = True, return_tensors = \"pt\", max_length=128))\n",
    "\n",
    "    return {'train': CustomDataset(encodings[0],y_train),\n",
    "            'validation': CustomDataset(encodings[1],y_val),\n",
    "            'test': CustomDataset(encodings[2],y_test),\n",
    "            'classes':class_names}\n",
    "\n",
    "def getNLI(path):\n",
    "    data_dict = {}\n",
    "    for i in ['train','dev','test']:\n",
    "        dataframe = pd.read_csv(path+'NLI/Roman Urdu/NLI.ru.{}.tsv'.format(i),sep='\\t')\n",
    "        sentences = dataframe[['premise','hypo']].to_numpy()\n",
    "        categorical = dataframe['Label'].astype('category').cat\n",
    "        labels = list(categorical.codes)\n",
    "        data = list(map(str.strip,sentences[:,0])),list(map(str.strip,sentences[:,1]))\n",
    "        encodings = tokenizer(*data,padding='max_length', truncation=True, add_special_tokens = True, return_attention_mask = True, return_tensors = \"pt\", max_length=128)\n",
    "        data_dict[i] = CustomDataset(encodings,labels)\n",
    "        if i == 'train':\n",
    "            data_dict['classes'] = list(categorical.categories)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def getNER(path):\n",
    "    return\n",
    "\n",
    "def getPOS(path):\n",
    "    return\n",
    "\n",
    "def getQuAD(path):\n",
    "    dataframe = pd.read_csv(path + 'QuAD/Roman Urdu/QuAD.ru.csv', sep=r\"\\s\\|\\s\", engine='python')\n",
    "    sentences = dataframe[[\"paragraph\", \"question\"]].to_numpy()\n",
    "    answers = dataframe[[\"answer starting idx\", \"answer\"]].to_numpy()\n",
    "\n",
    "    y = answer_to_idx(answers, sentences[:, 0])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125,random_state=1)  # 0.125 * 0.8 = 0.1\n",
    "\n",
    "    encodings = []\n",
    "    for data in [X_train, X_val, X_test]:\n",
    "        encodings.append(\n",
    "            tokenizer(list(data[:, 0]), list(data[:, 1]), padding='max_length', truncation='only_first', add_special_tokens=True,\n",
    "                      return_attention_mask=True,\n",
    "                      return_tensors=\"pt\", max_length=128))\n",
    "\n",
    "    return {'train': CustomDataset(char_to_token_position(encodings[0], y_train)),\n",
    "            'validation': CustomDataset(char_to_token_position(encodings[1], y_val)),\n",
    "            'test': CustomDataset(char_to_token_position(encodings[2], y_test))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: PYDEVD_USE_CYTHON environment variable is set to 'NO'. Frame evaluator will be also disabled because it requires Cython extensions to be enabled in order to operate correctly.\n"
     ]
    }
   ],
   "source": [
    "fine_tune_dataset = locals()['get'+TASK](DATA_ROOT)\n",
    "squad_metric = load_metric(\"squad\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    if TASK == 'QuAD':\n",
    "        answer_start_ids = np.argmax(pred.predictions[0],axis=-1)\n",
    "        answer_end_ids = np.argmax(pred.predictions[1],axis=-1)\n",
    "        return squad_metric.compute(predictions=list(zip(answer_start_ids,answer_end_ids))\n",
    "                                    ,references=list(zip(*pred.label_ids)))\n",
    "    else:\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='fine_tune_results/{}'.format(TASK),  # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=hyperparams['epochs'],  # total number of training epochs\n",
    "    per_device_train_batch_size=hyperparams['batch_size'],  # batch size per device during training\n",
    "    per_device_eval_batch_size=hyperparams['batch_size'],  # batch size for evaluation\n",
    "    warmup_steps=60,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir='./logs',  # directory for storing logs\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps = 10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=fine_tune_dataset['train'],  # training dataset\n",
    "    eval_dataset=fine_tune_dataset['validation'],  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/usama/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='2' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/39 : < :, Epoch 0.08/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "23",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-87ce0dfc0153>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# pydev_debug_cell\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001B[0m\n\u001B[1;32m   1170\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallback_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_step_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1171\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1172\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_log_save_evaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtr_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrial\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1173\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1174\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_epoch_stop\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_training_stop\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36m_maybe_log_save_evaluate\u001B[0;34m(self, tr_loss, model, trial, epoch)\u001B[0m\n\u001B[1;32m   1263\u001B[0m         \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1264\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_evaluate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1265\u001B[0;31m             \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1266\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_report_to_hp_search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1267\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mevaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   1762\u001B[0m             \u001B[0mprediction_loss_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_metrics\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1763\u001B[0m             \u001B[0mignore_keys\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mignore_keys\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1764\u001B[0;31m             \u001B[0mmetric_key_prefix\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmetric_key_prefix\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1765\u001B[0m         )\n\u001B[1;32m   1766\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mprediction_loop\u001B[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   1928\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1929\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_metrics\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mpreds\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlabel_ids\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1930\u001B[0;31m             \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_metrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mEvalPrediction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpreds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel_ids\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlabel_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1931\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1932\u001B[0m             \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-b12461fce30d>\u001B[0m in \u001B[0;36mcompute_metrics\u001B[0;34m(pred)\u001B[0m\n\u001B[1;32m      8\u001B[0m         \u001B[0manswer_end_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         return squad_metric.compute(predictions=list(zip(answer_start_ids,answer_end_ids))\n\u001B[0;32m---> 10\u001B[0;31m                                     ,references=list(zip(*pred.label_ids)))\n\u001B[0m\u001B[1;32m     11\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlabel_ids\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/datasets/metric.py\u001B[0m in \u001B[0;36mcompute\u001B[0;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    389\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mpredictions\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 390\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreferences\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreferences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    391\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_finalize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    392\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/datasets/metric.py\u001B[0m in \u001B[0;36madd_batch\u001B[0;34m(self, predictions, references)\u001B[0m\n\u001B[1;32m    429\u001B[0m         \"\"\"\n\u001B[1;32m    430\u001B[0m         \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m\"predictions\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"references\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mreferences\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 431\u001B[0;31m         \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    432\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwriter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    433\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_init_writer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/datasets/features.py\u001B[0m in \u001B[0;36mencode_batch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    958\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumn\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    959\u001B[0m             \u001B[0mcolumn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcast_to_python_objects\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 960\u001B[0;31m             \u001B[0mencoded_batch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mencode_nested_example\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcolumn\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    961\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mencoded_batch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    962\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/datasets/features.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    958\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumn\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    959\u001B[0m             \u001B[0mcolumn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcast_to_python_objects\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 960\u001B[0;31m             \u001B[0mencoded_batch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mencode_nested_example\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcolumn\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    961\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mencoded_batch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    962\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/datasets/features.py\u001B[0m in \u001B[0;36mencode_nested_example\u001B[0;34m(schema, obj)\u001B[0m\n\u001B[1;32m    845\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    846\u001B[0m         return {\n\u001B[0;32m--> 847\u001B[0;31m             \u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mencode_nested_example\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msub_schema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msub_obj\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0msub_schema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msub_obj\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzip_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    848\u001B[0m         }\n\u001B[1;32m    849\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/datasets/features.py\u001B[0m in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    844\u001B[0m     \u001B[0;31m# Nested structures: we allow dict, list/tuples, sequences\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    845\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 846\u001B[0;31m         return {\n\u001B[0m\u001B[1;32m    847\u001B[0m             \u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mencode_nested_example\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msub_schema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msub_obj\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0msub_schema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msub_obj\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzip_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    848\u001B[0m         }\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36mzip_dict\u001B[0;34m(*dicts)\u001B[0m\n\u001B[1;32m     96\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitertools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mdicts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# set merge all keys\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     97\u001B[0m         \u001B[0;31m# Will raise KeyError if the dict don't have the same keys\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 98\u001B[0;31m         \u001B[0;32myield\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdicts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Roman_Urdu_BERT/venv/lib/python3.6/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     96\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitertools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mdicts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# set merge all keys\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     97\u001B[0m         \u001B[0;31m# Will raise KeyError if the dict don't have the same keys\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 98\u001B[0;31m         \u001B[0;32myield\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdicts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 23"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(fine_tune_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('fine_tune_results/{}/best-checkpoint_f1'.format(TASK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}