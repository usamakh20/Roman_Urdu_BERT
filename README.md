# Multilingual, Bilingual and Monolingual BERT for Roman Urdu

1. Complete training code is present in BERT_ROMAN_URDU.ipynb
2. Use learning rate 2e-5 when starting from a pretrained model otherwise use 1e-4

### Experiments
1. Experiment with different vocabulary sizes (Note common tokens and performance)