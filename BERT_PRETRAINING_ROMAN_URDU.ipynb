{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U tokenizers\n",
    "!pip install tensorflow-gpu==1.15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_length = 50000\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --header=\"Host: codeload.github.com\" \\\n",
    "    --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\" \\\n",
    "    --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" \\\n",
    "    --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" \\\n",
    "    --header=\"Referer: https://github.com/google-research/bert\" \\\n",
    "    --header=\"Cookie: _octo=GH1.1.68793831.1588906101; _ga=GA1.2.19990328.1588906163; logged_in=no; _gat=1; tz=Asia%2FKarachi\" \\\n",
    "    --header=\"Connection: keep-alive\" \"https://codeload.github.com/google-research/bert/zip/master\" \\\n",
    "    -c -O 'bert-master.zip'\n",
    "\n",
    "!unzip bert-master.zip\n",
    "!rm bert-master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip \\\n",
    "    -c -O 'bert-base-uncased.zip'\n",
    "\n",
    "!unzip bert-base-uncased.zip -d bert-base-uncased\n",
    "!rm bert-base-uncased.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!gdown --id 1dRYeLV7NvcN2GmYb3X0CdX93juTM2rQi -O data/\n",
    "!gdown --id 102gHSTw_XxBs31XM6ZGe4KkVfV0VuSLy -O data/\n",
    "!gdown --id 1OxvR5pdR5CgHBotf2YuLM0-_vFNe6u6a -O data/\n",
    "!gdown --id 1Pofed4RbRlCQiDmv4MjNM0ogus0XErBB -O data/\n",
    "!gdown --id 1tE8f4-c0ZqYQKNiAfEnfojcsZIxnvEzU -O data/\n",
    "!gdown --id 1eTq3ngxff0Npt1for_i8iBgD66hvaNx8 -O data/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob('data/' + '*')\n",
    "\n",
    "text_data = []\n",
    "for file in files:\n",
    "    with open(file, 'r') as data:\n",
    "        text = list(filter(lambda x: x != '\\n', data.readlines()))\n",
    "        text_data.append(''.join(text))\n",
    "\n",
    "with open('all_data.txt','w') as f:\n",
    "    f.write('\\n'.join(text_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "\n",
    "roman_BWPT = tokenizers.BertWordPieceTokenizer(\n",
    "    # add_special_tokens=True, # This argument doesn't work in the latest version of BertWordPieceTokenizer\n",
    "    unk_token='[UNK]',\n",
    "    sep_token='[SEP]',\n",
    "    cls_token='[CLS]',\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=True,\n",
    "    lowercase=True,\n",
    "    wordpieces_prefix='##'\n",
    ")\n",
    "\n",
    "roman_BWPT.train(\n",
    "    files=[\"all_data.txt\"],\n",
    "    vocab_size=vocab_length,\n",
    "    min_frequency=3,\n",
    "    limit_alphabet=1000,\n",
    "    show_progress=True,\n",
    "    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[MASK]', '[SEP]']\n",
    ")\n",
    "\n",
    "roman_BWPT.save_model(\".\", \"roman-urdu-vocab-uncased_\"+str(vocab_length))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "english_vocab = open('bert-base-uncased/vocab.txt', 'r').read().split('\\n')[:-1]\n",
    "roman_urdu_vocab = open('roman-urdu-vocab-uncased_'+str(vocab_length)+'-vocab.txt', 'r').read().split('\\n')[:-1]\n",
    "\n",
    "common_vocab = list(set(english_vocab).intersection(set(roman_urdu_vocab)))\n",
    "print('No. of common tokens: ',len(common_vocab))\n",
    "\n",
    "augmented_vocab = [''] * len(roman_urdu_vocab)\n",
    "\n",
    "for vocab in common_vocab:\n",
    "    augmented_vocab[english_vocab.index(vocab)] = vocab\n",
    "    roman_urdu_vocab.pop(roman_urdu_vocab.index(vocab))\n",
    "\n",
    "for i in range(len(augmented_vocab)):\n",
    "    if augmented_vocab[i] == '':\n",
    "        augmented_vocab[i] = roman_urdu_vocab.pop(0)\n",
    "\n",
    "with open('augmented_vocab.txt', 'w') as v:\n",
    "    v.write('\\n'.join(augmented_vocab))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BERT_BASE_DIR='bert-base-uncased'\n",
    "\n",
    "import json\n",
    "with open(BERT_BASE_DIR+'/bert_config.json', \"r+\") as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "    data[\"vocab_size\"] = sum(1 for line in open('augmented_vocab.txt'))\n",
    "    jsonFile.seek(0)  # rewind\n",
    "    json.dump(data, jsonFile)\n",
    "    jsonFile.truncate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use this instead of below command if less than 64GB RAM\n",
    "!mkdir data_parts pretraining_data\n",
    "!split -C 100m --numeric-suffixes all_data.txt data_parts/all_data\n",
    "!gdown --id 13UCnkCcLO30aw03n9t1chlYXybifuOeB\n",
    "!chmod +x create_pretraining.sh\n",
    "!./create_pretraining.sh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python bert-master/create_pretraining_data.py \\\n",
    "    --input_file all_data.txt \\\n",
    "    --output_file tf_examples_multi.tfrecord \\\n",
    "    --vocab_file augmented_vocab.txt \\\n",
    "    --do_lower_case True \\\n",
    "    --max_seq_length 128 \\\n",
    "    --max_predictions_per_seq 20 \\\n",
    "    --masked_lm_prob 0.15 \\\n",
    "    --random_seed 42 \\\n",
    "    --dupe_factor 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python bert-master/run_pretraining.py \\\n",
    "    --input_file=tf_examples_multi.tfrecord \\\n",
    "    --output_dir=bert_bilingual_roman_urdu \\\n",
    "    --do_train=True \\\n",
    "    --do_eval=True \\\n",
    "    --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n",
    "    --train_batch_size=32 \\\n",
    "    --max_seq_length=128 \\\n",
    "    --max_predictions_per_seq=20 \\\n",
    "    --num_train_steps=100000 \\\n",
    "    --num_warmup_steps=10 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}