{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in ./venv/lib/python3.6/site-packages (0.10.1)\r\n",
      "Collecting gdown\r\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\r\n",
      "  Installing build dependencies ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25h    Preparing wheel metadata ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25hRequirement already satisfied: six in ./venv/lib/python3.6/site-packages (from gdown) (1.15.0)\r\n",
      "Collecting filelock\r\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\r\n",
      "Collecting tqdm\r\n",
      "  Downloading tqdm-4.56.2-py2.py3-none-any.whl (72 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 72 kB 415 kB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting requests[socks]\r\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\r\n",
      "Collecting certifi>=2017.4.17\r\n",
      "  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\r\n",
      "Collecting idna<3,>=2.5\r\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\r\n",
      "Collecting urllib3<1.27,>=1.21.1\r\n",
      "  Downloading urllib3-1.26.3-py2.py3-none-any.whl (137 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 137 kB 1.1 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting chardet<5,>=3.0.2\r\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\r\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\r\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\r\n",
      "Building wheels for collected packages: gdown\r\n",
      "  Building wheel for gdown (PEP 517) ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25h  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9681 sha256=3dfdbe2e45df57871457ee34335b2459d7c8b6e60e652c84fdfdacee3962da14\r\n",
      "  Stored in directory: /home/usama/.cache/pip/wheels/33/15/6e/df5f8336275e96e19599034a76f9cfd81c6ae15d2bf16c11ca\r\n",
      "Successfully built gdown\r\n",
      "Installing collected packages: urllib3, idna, chardet, certifi, requests, PySocks, tqdm, filelock, gdown\r\n",
      "Successfully installed PySocks-1.7.1 certifi-2020.12.5 chardet-4.0.0 filelock-3.0.12 gdown-3.12.2 idna-2.10 requests-2.25.1 tqdm-4.56.2 urllib3-1.26.3\r\n",
      "Requirement already satisfied: tensorflow-gpu==1.15 in ./venv/lib/python3.6/site-packages (1.15.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (1.1.2)\r\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (1.15.1)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (1.1.0)\r\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (1.35.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (0.36.2)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (0.11.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (0.2.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (3.3.0)\r\n",
      "Requirement already satisfied: protobuf>=3.6.1 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (3.14.0)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (1.0.8)\r\n",
      "Requirement already satisfied: astor>=0.6.0 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (0.8.1)\r\n",
      "Requirement already satisfied: gast==0.2.2 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (0.2.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (1.19.5)\r\n",
      "Requirement already satisfied: six>=1.10.0 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (1.15.0)\r\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (1.15.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./venv/lib/python3.6/site-packages (from tensorflow-gpu==1.15) (1.12.1)\r\n",
      "Requirement already satisfied: h5py in ./venv/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (3.1.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.3)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (53.0.0)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./venv/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\r\n",
      "Requirement already satisfied: importlib-metadata in ./venv/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.0)\r\n",
      "Requirement already satisfied: cached-property in ./venv/lib/python3.6/site-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15) (1.5.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./venv/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.7.4.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tokenizers\n",
    "!pip install gdown\n",
    "!pip install tensorflow-gpu==1.15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "vocab_length = 30522\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --header=\"Host: codeload.github.com\" \\\n",
    "    --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\" \\\n",
    "    --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" \\\n",
    "    --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" \\\n",
    "    --header=\"Referer: https://github.com/google-research/bert\" \\\n",
    "    --header=\"Cookie: _octo=GH1.1.68793831.1588906101; _ga=GA1.2.19990328.1588906163; logged_in=no; _gat=1; tz=Asia%2FKarachi\" \\\n",
    "    --header=\"Connection: keep-alive\" \"https://codeload.github.com/google-research/bert/zip/master\" \\\n",
    "    -c -O 'bert-master.zip'\n",
    "\n",
    "!unzip bert-master.zip\n",
    "!rm bert-master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip -c -O 'bert-multilingual-base-uncased.zip'\n",
    "!wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip \\\n",
    "    -c -O 'bert-base-uncased.zip'\n",
    "\n",
    "!unzip bert-base-uncased.zip -d bert-base-uncased\n",
    "!rm bert-base-uncased.zip\n",
    "!unzip bert-multilingual-base-uncased.zip -d bert-multilingual-base-uncased\n",
    "!rm bert-multilingual-base-uncased.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!gdown --id 1dRYeLV7NvcN2GmYb3X0CdX93juTM2rQi -O data/\n",
    "!gdown --id 102gHSTw_XxBs31XM6ZGe4KkVfV0VuSLy -O data/\n",
    "!gdown --id 1OxvR5pdR5CgHBotf2YuLM0-_vFNe6u6a -O data/\n",
    "!gdown --id 1Pofed4RbRlCQiDmv4MjNM0ogus0XErBB -O data/\n",
    "!gdown --id 1tE8f4-c0ZqYQKNiAfEnfojcsZIxnvEzU -O data/\n",
    "!gdown --id 1eTq3ngxff0Npt1for_i8iBgD66hvaNx8 -O data/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob('data/' + '*')\n",
    "\n",
    "text_data = []\n",
    "for file in files:\n",
    "    with open(file, 'r') as data:\n",
    "        text = list(filter(lambda x: x != '\\n', data.readlines()))\n",
    "        text_data.append(''.join(text))\n",
    "\n",
    "with open('all_data.txt','w') as f:\n",
    "    f.write('\\n'.join(text_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['./roman-urdu-vocab-uncased_30522-vocab.txt']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "\n",
    "roman_BWPT = tokenizers.BertWordPieceTokenizer(\n",
    "    # add_special_tokens=True, # This argument doesn't work in the latest version of BertWordPieceTokenizer\n",
    "    unk_token='[UNK]',\n",
    "    sep_token='[SEP]',\n",
    "    cls_token='[CLS]',\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=True,\n",
    "    lowercase=True,\n",
    "    wordpieces_prefix='##'\n",
    ")\n",
    "\n",
    "roman_BWPT.train(\n",
    "    files=[\"all_data.txt\"],\n",
    "    vocab_size=vocab_length,\n",
    "    min_frequency=3,\n",
    "    limit_alphabet=1000,\n",
    "    show_progress=True,\n",
    "    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[MASK]', '[SEP]']\n",
    ")\n",
    "\n",
    "roman_BWPT.save_model(\".\", \"roman-urdu-vocab-uncased_\"+str(vocab_length))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of common tokens:  7586\n"
     ]
    }
   ],
   "source": [
    "english_vocab = open('bert-base-uncased/vocab.txt', 'r').read().split('\\n')[:-1]\n",
    "roman_urdu_vocab = open('roman-urdu-vocab-uncased_'+str(vocab_length)+'-vocab.txt', 'r').read().split('\\n')[:-1]\n",
    "\n",
    "common_vocab = list(set(english_vocab).intersection(set(roman_urdu_vocab)))\n",
    "print('No. of common tokens: ',len(common_vocab))\n",
    "\n",
    "augmented_vocab = [''] * len(roman_urdu_vocab)\n",
    "\n",
    "for vocab in common_vocab:\n",
    "    augmented_vocab[english_vocab.index(vocab)] = vocab\n",
    "    roman_urdu_vocab.pop(roman_urdu_vocab.index(vocab))\n",
    "\n",
    "for i in range(len(augmented_vocab)):\n",
    "    if augmented_vocab[i] == '':\n",
    "        augmented_vocab[i] = roman_urdu_vocab.pop(0)\n",
    "\n",
    "with open('augmented_vocab.txt', 'w') as v:\n",
    "    v.write('\\n'.join(augmented_vocab))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#\n",
    "# import json\n",
    "# with open(BERT_BASE_DIR+'/bert_config.json', \"r+\") as jsonFile:\n",
    "#     data = json.load(jsonFile)\n",
    "#     data[\"vocab_size\"] = sum(1 for line in open('augmented_vocab.txt'))\n",
    "#     jsonFile.seek(0)  # rewind\n",
    "#     json.dump(data, jsonFile)\n",
    "#     jsonFile.truncate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=13UCnkCcLO30aw03n9t1chlYXybifuOeB\r\n",
      "To: /home/usama/PycharmProjects/Roman_Urdu_BERT/create_pretraining.sh\r\n",
      "100%|██████████████████████████████████████████| 463/463 [00:00<00:00, 1.72MB/s]\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR='model_multilingual_original_vocab'\n",
    "# use this instead of below command if less than 64GB RAM\n",
    "!mkdir data_parts pretraining_data\n",
    "!split -C 50m --numeric-suffixes all_data.txt data_parts/all_data\n",
    "!gdown --id 13UCnkCcLO30aw03n9t1chlYXybifuOeB\n",
    "!chmod +x create_pretraining.sh\n",
    "!./create_pretraining.sh 128 $MODEL_DIR/vocab.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python bert-master/create_pretraining_data.py \\\n",
    "    --input_file all_data.txt \\\n",
    "    --output_file tf_examples_multi.tfrecord \\\n",
    "    --vocab_file augmented_vocab.txt \\\n",
    "    --do_lower_case True \\\n",
    "    --max_seq_length 128 \\\n",
    "    --max_predictions_per_seq 20 \\\n",
    "    --masked_lm_prob 0.15 \\\n",
    "    --random_seed 42 \\\n",
    "    --dupe_factor 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BERT_BASE_DIR='bert-multilingual-base-uncased'\n",
    "!python bert-master/run_pretraining.py \\\n",
    "    --input_file=pretraining_data/tf_examples_multi.tfrecord* \\\n",
    "    --output_dir=$MODEL_DIR \\\n",
    "    --do_train=True \\\n",
    "    --do_eval=True \\\n",
    "    --bert_config_file=$MODEL_DIR/config.json \\\n",
    "    --train_batch_size=16 \\\n",
    "    --max_seq_length=128 \\\n",
    "    --max_predictions_per_seq=20 \\\n",
    "    --num_train_steps=100000 \\\n",
    "    --num_warmup_steps=10000 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}