{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22 22:38:55.171740: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n",
      "2021-04-22 22:38:55.171838: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "Building PyTorch model from configuration: BertConfig {\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"directionality\": \"bidi\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"pooler_fc_size\": 768,\r\n",
      "  \"pooler_num_attention_heads\": 12,\r\n",
      "  \"pooler_num_fc_layers\": 3,\r\n",
      "  \"pooler_size_per_head\": 128,\r\n",
      "  \"pooler_type\": \"first_token_transform\",\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.1\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 105879\r\n",
      "}\r\n",
      "\r\n",
      "Converting TensorFlow checkpoint from /home/usama/PycharmProjects/Roman_Urdu_BERT/model_multilingual_vocab_extension_400000_steps/model.ckpt-400000\r\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\r\n",
      "Loading TF weight bert/embeddings/position_embeddings/adam_m with shape [512, 768]\r\n",
      "Loading TF weight bert/embeddings/position_embeddings/adam_v with shape [512, 768]\r\n",
      "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\r\n",
      "Loading TF weight bert/embeddings/token_type_embeddings/adam_m with shape [2, 768]\r\n",
      "Loading TF weight bert/embeddings/token_type_embeddings/adam_v with shape [2, 768]\r\n",
      "Loading TF weight bert/embeddings/word_embeddings with shape [105879, 768]\r\n",
      "2021-04-22 22:39:06.723096: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 325260288 exceeds 10% of free system memory.\r\n",
      "Loading TF weight bert/embeddings/word_embeddings/adam_m with shape [105879, 768]\r\n",
      "2021-04-22 22:39:08.756158: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 325260288 exceeds 10% of free system memory.\r\n",
      "Loading TF weight bert/embeddings/word_embeddings/adam_v with shape [105879, 768]\r\n",
      "2021-04-22 22:39:11.582704: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 325260288 exceeds 10% of free system memory.\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_m with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_v with shape [3072]\r\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [768, 3072]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_m with shape [3072, 768]\r\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_v with shape [3072, 768]\r\n",
      "Loading TF weight bert/pooler/dense/bias with shape [768]\r\n",
      "Loading TF weight bert/pooler/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight bert/pooler/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight bert/pooler/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight bert/pooler/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight cls/predictions/output_bias with shape [105879]\r\n",
      "Loading TF weight cls/predictions/output_bias/adam_m with shape [105879]\r\n",
      "Loading TF weight cls/predictions/output_bias/adam_v with shape [105879]\r\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\r\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_m with shape [768]\r\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_v with shape [768]\r\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\r\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_m with shape [768]\r\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_v with shape [768]\r\n",
      "Loading TF weight cls/predictions/transform/dense/bias with shape [768]\r\n",
      "Loading TF weight cls/predictions/transform/dense/bias/adam_m with shape [768]\r\n",
      "Loading TF weight cls/predictions/transform/dense/bias/adam_v with shape [768]\r\n",
      "Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\r\n",
      "Loading TF weight cls/predictions/transform/dense/kernel/adam_m with shape [768, 768]\r\n",
      "Loading TF weight cls/predictions/transform/dense/kernel/adam_v with shape [768, 768]\r\n",
      "Loading TF weight cls/seq_relationship/output_bias with shape [2]\r\n",
      "Loading TF weight cls/seq_relationship/output_bias/adam_m with shape [2]\r\n",
      "Loading TF weight cls/seq_relationship/output_bias/adam_v with shape [2]\r\n",
      "Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\r\n",
      "Loading TF weight cls/seq_relationship/output_weights/adam_m with shape [2, 768]\r\n",
      "Loading TF weight cls/seq_relationship/output_weights/adam_v with shape [2, 768]\r\n",
      "Loading TF weight global_step with shape []\r\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/embeddings/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/embeddings/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/embeddings/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/embeddings/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\r\n",
      "Skipping bert/embeddings/position_embeddings/adam_m\r\n",
      "Skipping bert/embeddings/position_embeddings/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\r\n",
      "Skipping bert/embeddings/token_type_embeddings/adam_m\r\n",
      "Skipping bert/embeddings/token_type_embeddings/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\r\n",
      "Skipping bert/embeddings/word_embeddings/adam_m\r\n",
      "Skipping bert/embeddings/word_embeddings/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_0/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_0/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_0/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_0/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_0/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_0/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_0/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_0/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_0/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_0/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_1/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_1/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_1/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_1/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_1/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_1/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_1/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_1/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_1/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_1/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_10/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_10/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_10/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_10/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_10/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_10/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_10/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_10/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_10/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_10/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_11/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_11/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_11/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_11/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_11/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_11/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_11/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\r\n",
      "Skipping bert/pooler/dense/bias/adam_m\r\n",
      "Skipping bert/pooler/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\r\n",
      "Skipping bert/pooler/dense/kernel/adam_m\r\n",
      "Skipping bert/pooler/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\r\n",
      "Skipping cls/predictions/output_bias/adam_m\r\n",
      "Skipping cls/predictions/output_bias/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\r\n",
      "Skipping cls/predictions/transform/LayerNorm/beta/adam_m\r\n",
      "Skipping cls/predictions/transform/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\r\n",
      "Skipping cls/predictions/transform/LayerNorm/gamma/adam_m\r\n",
      "Skipping cls/predictions/transform/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\r\n",
      "Skipping cls/predictions/transform/dense/bias/adam_m\r\n",
      "Skipping cls/predictions/transform/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\r\n",
      "Skipping cls/predictions/transform/dense/kernel/adam_m\r\n",
      "Skipping cls/predictions/transform/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\r\n",
      "Skipping cls/seq_relationship/output_bias/adam_m\r\n",
      "Skipping cls/seq_relationship/output_bias/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\r\n",
      "Skipping cls/seq_relationship/output_weights/adam_m\r\n",
      "Skipping cls/seq_relationship/output_weights/adam_v\r\n",
      "Skipping global_step\r\n",
      "Save PyTorch model to model_multilingual_vocab_extension_400000_steps/pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = 'model_multilingual_vocab_extension_400000_steps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!transformers-cli convert --model_type bert \\\n",
    "  --tf_checkpoint $MODEL_DIR/model.ckpt-400000 \\\n",
    "  --config $MODEL_DIR/config.json \\\n",
    "  --pytorch_dump_output $MODEL_DIR/pytorch_model.bin"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_DIR, num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}